{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9fhnoiEgkoQ",
        "outputId": "be6b0651-68eb-471f-f00d-76100fe31f23"
      },
      "source": [
        "# import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clFMft0-g1YH"
      },
      "source": [
        "df = pd.read_csv('/content/clean_tweets.csv')\n",
        "\n",
        "# Removing the unnecessary columns.\n",
        "df = df[['sentiment','text']]\n",
        "df.text=df.text.astype(str)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO1s6uMChW3R",
        "outputId": "9edd99df-dcdf-4b1a-ec75-f5e30ac64147"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    0\n",
              "text         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Gw1JU-xPhi-O",
        "outputId": "12afb4c2-7f27-4dd1-ec03-1b6f3ea2fa40"
      },
      "source": [
        "df.dropna()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>so i m going to start a new trend instead of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>edgefest or maybe since you re driving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>i need something exciting to happen xo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>what happened last night i wasnt too impressed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i hate my nose from the front</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159820</th>\n",
              "      <td>1</td>\n",
              "      <td>thank you she is so great more bragging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159821</th>\n",
              "      <td>0</td>\n",
              "      <td>it s cold out no beach today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159822</th>\n",
              "      <td>0</td>\n",
              "      <td>yo facebook is porno n n haha im playin but th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159823</th>\n",
              "      <td>1</td>\n",
              "      <td>tis a pink day today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159824</th>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159825 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentiment                                               text\n",
              "0               1  so i m going to start a new trend instead of t...\n",
              "1               0             edgefest or maybe since you re driving\n",
              "2               0             i need something exciting to happen xo\n",
              "3               0  what happened last night i wasnt too impressed...\n",
              "4               0                      i hate my nose from the front\n",
              "...           ...                                                ...\n",
              "159820          1            thank you she is so great more bragging\n",
              "159821          0                       it s cold out no beach today\n",
              "159822          0  yo facebook is porno n n haha im playin but th...\n",
              "159823          1                               tis a pink day today\n",
              "159824          1                                                nan\n",
              "\n",
              "[159825 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAHMwtHehq-r"
      },
      "source": [
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "## set containing all stopwords.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "##Function to clean the data.\n",
        "def preprocess(textdata , wordLemm):\n",
        "    processedText = []\n",
        "    \n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "    for tweet in textdata:\n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        # Replace all URls with 'URL'\n",
        "        tweet = re.sub(urlPattern,' URL',tweet)\n",
        "        # Replace all emojis.\n",
        "        for emoji in emojis.keys():\n",
        "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "        # Replace @USERNAME to 'USER'.\n",
        "        tweet = re.sub(userPattern,' USER', tweet)        \n",
        "        # Replace all non alphabets.\n",
        "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "        # Replace 3 or more consecutive letters by 2 letter.\n",
        "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "        \n",
        "        tweetwords = ''\n",
        "        for word in tweet.split():\n",
        "            if len(word)>1:\n",
        "                # Lemmatizing the word.\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                tweetwords += (word+' ')\n",
        "            \n",
        "        processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText\n",
        "\n",
        "text, sentiment = list(df['text']), list(df['sentiment'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuonT2udh3pz"
      },
      "source": [
        "wordLemm = WordNetLemmatizer()\n",
        "processedtext = preprocess(text , wordLemm)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JlS9w7Limgo"
      },
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(processedtext , sentiment , train_size = 0.8 , test_size = 0.2 , random_state = 0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfWdH-F8ir0n",
        "outputId": "229104f9-1542-4082-cba0-557712ccbbe5"
      },
      "source": [
        "pd.DataFrame(X_train , y_train).info"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of                                                     0\n",
              "1   hungry hungry hippo lol ve been vegeterian fer...\n",
              "1                                 what wa the advice \n",
              "0   me too they were kind of cute stuntling there ...\n",
              "1   do you have adventure trip photo up and take p...\n",
              "1                             just fixed my computer \n",
              "..                                                ...\n",
              "0             it official hand broke got big as cast \n",
              "1                          no nyc see you in la then \n",
              "0                           so dead tired once again \n",
              "1   just watched the st episode of nurse jackie on...\n",
              "1   aahh finally an avatar in case you are interes...\n",
              "\n",
              "[127860 rows x 1 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYXJd_Skixxn"
      },
      "source": [
        "#incode the object columns by Tfidf\n",
        "Incoder = TfidfVectorizer(ngram_range=(1,2), max_features=1000000)\n",
        "Incoder.fit(X_train)\n",
        "\n",
        "X_train = Incoder.transform(X_train)\n",
        "X_test  = Incoder.transform(X_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chvFcpP6i29C"
      },
      "source": [
        "#Function to compare between my models to chose one\n",
        "def model_Evaluate(model):\n",
        "    \n",
        "    # Predict values for Test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsKLB_gIi6GV",
        "outputId": "36e78cfd-6358-436d-994b-4e20180956c0"
      },
      "source": [
        "SVCmodel = LinearSVC()\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "model_Evaluate(SVCmodel)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.80      0.80     16057\n",
            "           1       0.80      0.79      0.79     15908\n",
            "\n",
            "    accuracy                           0.79     31965\n",
            "   macro avg       0.79      0.79      0.79     31965\n",
            "weighted avg       0.79      0.79      0.79     31965\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEswZgf4i-BY",
        "outputId": "e91dc553-2f09-4cf0-90f7-7c4934def11b"
      },
      "source": [
        "LogisticModel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
        "LogisticModel.fit(X_train, y_train)\n",
        "model_Evaluate(LogisticModel)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80     16057\n",
            "           1       0.80      0.79      0.80     15908\n",
            "\n",
            "    accuracy                           0.80     31965\n",
            "   macro avg       0.80      0.80      0.80     31965\n",
            "weighted avg       0.80      0.80      0.80     31965\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67wcqiEbjNVS"
      },
      "source": [
        "#Logistic regression did better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N57sbGJDjRE-"
      },
      "source": [
        "# Saving the model and the incoder\n",
        "file = open('Sentiment-LR-model.pickle','wb')\n",
        "pickle.dump(LogisticModel, file)\n",
        "file.close()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DLypUAejbF4"
      },
      "source": [
        "file = open('Incoder-ngram-(1,2).pickle','wb')\n",
        "pickle.dump(Incoder, file)\n",
        "file.close()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58gBB8w1jfGr",
        "outputId": "4429fec5-9e08-4aaf-c5ce-15358d46b818"
      },
      "source": [
        "predictions = LogisticModel.predict(X_test)\n",
        "\n",
        "output = pd.DataFrame({'text': X_test, 'sentiment': predictions})\n",
        "output.to_csv('my_submission.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission was successfully saved!\n"
          ]
        }
      ]
    }
  ]
}